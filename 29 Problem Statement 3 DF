
val crimeData = sc.textFile("/user/cloudera/crimesdataset/crimes.csv")
// display top 10 records
crimeData.take(10).foreach(println)
// ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location
//4647369,HM155213,01/31/2006 12:13:05 PM,066XX N BOSWORTH AVE,1811,NARCOTICS,POSS: CANNABIS 30GMS OR LESS,"SCHOOL, PUBLIC, BUILDING",true,false,2432,024,40,1,18,1164737,1944193,2006,04/15/2016 08:55:02 AM,42.002478396,-87.66929687,"(42.002478396, -87.66929687)"
// we need to exclude the header from the dataset

val header = crimeData.first
val crimeDataWithoutHeader = crimeData.filter(criminalRecord => criminalRecord != header)

val crimeDataWithoutHeaderDF = crimeDataWithoutHeader.
  map(rec => {
    val r = rec.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)
    (r(7), r(5))
}).toDF("location_description", "crime_type")

crimeDataWithoutHeaderDF.registerTempTable("crime_data")

// set number of partitions to 4 because we don't need so many parallel threads, default is 200
sqlContext.setConf("spark.sql.shuffle.partitions", "4") 
sqlContext.sql("select * from (" +
   "select crime_type, count(1) as crime_count " + 
   "from crime_data " + 
   "where location_description = 'RESIDENCE' " + 
   "group by crime_type " + 
   "order by crime_count desc) q limit 3").
coalesce(1).
save("/user/cloudera/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA_DF", "json")

